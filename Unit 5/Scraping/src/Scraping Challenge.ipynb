{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own. Pick a new website and see what you can get out of it. Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.\n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1. Return specific pieces of information (rather than just downloading a whole page)\n",
    "2. Iterate over multiple pages/queries\n",
    "3. Save the data to your computer\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest. Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Website\n",
    "\n",
    "What if I use glassdoor to get company information to prepare for an interview?\n",
    "1. All interview questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Imports ##############################\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "class GlassdoorSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.glassdoor.com/Interview/Facebook-Interview-Questions-E40772.htm?filter.jobTitleFTS=Data+Scientist',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for review in response.xpath('//empReview cf'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "#                 'name': review.xpath('header/h2/a/@title').extract_first(),\n",
    "#                 'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'interviewquestions': review.xpath('section[@class=\"interviewQuestions\"]/p/text()').extract(),\n",
    "#                 'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "        # Get the URL of the previous page.\n",
    "#         next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "        \n",
    "#         # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
    "#         # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
    "#         pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "#         # Recursively call the spider to run on the next page, if it exists.\n",
    "#         if next_page is not None and pagenum < 10:\n",
    "#             next_page = response.urljoin(next_page)\n",
    "#             # Request the next page and recursively parse it the same way we did above\n",
    "#             yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'data.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(GlassdoorSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://www.glassdoor.com/Interview/Facebook-Interview-Questions-E40772.htm?filter.jobTitleFTS=Data+Scientist>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "<Selector xpath='//*[starts-with(@id, \"InterviewReview_\")]' data='<li class=\" empReview cf \" id=\"Interview'>\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#Quick Test - kind of works but trying a different approach\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class GlassdoorSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"Glassdoor_Simple\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.glassdoor.com/Interview/Facebook-Interview-Questions-E40772.htm?filter.jobTitleFTS=Data+Scientist',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        print(response)\n",
    "        # Iterate over every review class element on the page.\n",
    "        count=0 \n",
    "        # This kind of works but want to try something else\n",
    "        for review in response.xpath('//*[starts-with(@id, \"InterviewReview_\")]'):\n",
    "            \n",
    "            print(review)\n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "#                 # This is the code to choose what we want to extract\n",
    "#                 # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'interviewquestions': review.xpath('//*[starts-with(@id, \"InterviewReview_\")]/div[3]/div/div[2]/div[2]/div/div').extract_first(),\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False ,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(GlassdoorSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://www.glassdoor.com/Interview/Facebook-Interview-Questions-E40772.htm?filter.jobTitleFTS=Data+Scientist>\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#@ This one works!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class GlassdoorSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"Glassdoor_Simple\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.glassdoor.com/Interview/Facebook-Interview-Questions-E40772.htm?filter.jobTitleFTS=Data+Scientist',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        print(response)\n",
    "        # Iterate over every review class element on the page.\n",
    "        count=0 \n",
    "        # Get all the reviews objects for the page\n",
    "        reviews = response.xpath('//*[starts-with(@id, \"InterviewReview_\")]')\n",
    "        #Extract the different infomration\n",
    "        for review in response.xpath('//*[starts-with(@id, \"InterviewReview_\")]'):\n",
    "            \n",
    "            #print(review.extract())\n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "#                 # This is the code to choose what we want to extract\n",
    "#                 # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'interviewquestions': review.xpath('.//div[3]/div/div[2]/div[2]/div/div/ul/li/span/text()').extract(),\n",
    "            }\n",
    "#//*[@id=\"InterviewReview_10544127\"]/div[3]/div/div[2]/div[2]/div/div/ul/li/span/text()[1]\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False ,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(GlassdoorSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
