{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Build your own NLP Model\n",
    "\n",
    "Will be predicting Sentiment on Tweets. Will be using a dataset from Sentiment140 which contains 1.6 million labeled tweets.\n",
    "\n",
    "1. 0 — the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "2. 1 — the id of the tweet (2087)\n",
    "3. 2 — the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. 3 — the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. 4 — the user that tweeted (robotickilldozr)\n",
    "6. 5 — the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Imports ####################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import boto3\n",
    "import io\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "####### Models ########\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 6.674104928970337 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#################################### Bring in Data #############################################\n",
    "start_time = time.time()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#Bring in Training Data\n",
    "obj = s3.get_object(Bucket='data-science-project-data', Key='Twitter_Sentiment_Analysis/training.1600000.processed.noemoticon.csv')\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "tweets = pd.read_csv(io.BytesIO(obj['Body'].read()),header=None, names=cols, encoding = \"ISO-8859-1\")\n",
    "#train.set_index('bidder_id', inplace=True)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Very Well Balanced\n",
    "tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 4.775571584701538 seconds ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58b6e9d320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Is there any missing data? - No\n",
    "plt.figure(figsize=(16,8))\n",
    "start_time = time.time()\n",
    "sns.heatmap(tweets.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Need the Sentiment and the Text\n",
    "tweets.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 16.67865300178528 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "tweets['Clean'] = tweets['text'].apply(lambda x: clean_tweet(x))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>Awww that s a bummer You shoulda got David Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can t update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball Managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  \\\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          0  is upset that he can't update his Facebook by ...   \n",
       "2          0  @Kenichan I dived many times for the ball. Man...   \n",
       "3          0    my whole body feels itchy and like its on fire    \n",
       "4          0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                               Clean  \n",
       "0  Awww that s a bummer You shoulda got David Car...  \n",
       "1  is upset that he can t update his Facebook by ...  \n",
       "2  I dived many times for the ball Managed to sav...  \n",
       "3     my whole body feels itchy and like its on fire  \n",
       "4  no it s not behaving at all i m mad why am i h...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Awww that s a bummer You shoulda got David Car...\n",
       "1    is upset that he can t update his Facebook by ...\n",
       "2    I dived many times for the ball Managed to sav...\n",
       "3       my whole body feels itchy and like its on fire\n",
       "4    no it s not behaving at all i m mad why am i h...\n",
       "5                                   not the whole crew\n",
       "6                                           Need a hug\n",
       "7    hey long time no see Yes Rains a bit only a bi...\n",
       "8                           K nope they didn t have it\n",
       "9                                         que me muera\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Clean'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Down Sample The data to evaluate only a small portion to speed up the time it takes to evaluate different models. \n",
    "# Once the highest performing model is found, then can train on the fuller dataset. and Create pipeline for the final model.\n",
    "tweets_subsampled_1, tweets_subsampled_2 = train_test_split(tweets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_subsampled_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    80122\n",
       "0    79878\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_subsampled_2['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between outcome and Features\n",
    "y = tweets_subsampled_2['sentiment']\n",
    "X = tweets_subsampled_2['Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 23.724626302719116 seconds ---\n",
      "Number of features: 110743\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the Clean tweets. This also invovled using the lemmatizer from Spacy.\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            ngram_range=(1,2), # Add 2 word phrases\n",
    "                             tokenizer=custom_tokenizer\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "tweets_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Number of features: %d\" % tweets_tfidf.get_shape()[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 11.66292929649353 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space to 2000.\n",
    "start_time = time.time()\n",
    "svd= TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the tweets data, then project the training data.\n",
    "tweets_lsa = lsa.fit_transform(tweets_tfidf)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_lsa,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Will need to take a subset of the data to try different models on, and then train on the full set. Will try several models:\n",
    "\n",
    "1. Logistic Regression (Ridge and Lasso)\n",
    "2. Stoachastic Gradient Descent (SGD with Hinge - SVM)\n",
    "3. Gradient Boosted Tree (Will take considerably long time)\n",
    "4. Random Forest. (Will take a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7057083333333334\n",
      "-- Execution time: 10.045667171478271 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################ Logistic Regression ######################\n",
    "start_time = time.time()\n",
    "parameters = {\n",
    "                'penalty':['l1'],\n",
    "                'C':[1],\n",
    "                'class_weight':['balanced']\n",
    "               \n",
    "              }\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(lr, parameters, scoring='accuracy', cv=3, verbose=0)\n",
    "#Fit the Data\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.67      0.69     23884\n",
      "          4       0.69      0.74      0.72     24116\n",
      "\n",
      "avg / total       0.71      0.71      0.71     48000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_prediction = grid.predict(X_test)\n",
    "print(classification_report(y_test, log_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7497083333333333\n",
      "-- Execution time: 48.20784306526184 seconds ---\n"
     ]
    }
   ],
   "source": [
    "###################### SGD #############################\n",
    "start_time = time.time()\n",
    "parameters = {\n",
    "                'loss':['log'],\n",
    "                'penalty':['l2'],\n",
    "                'fit_intercept':[True],\n",
    "                'class_weight':['balanced']\n",
    "               \n",
    "              }\n",
    "\n",
    "clf = linear_model.SGDClassifier(n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(clf, parameters, scoring='accuracy', cv=5, verbose=0)\n",
    "#Fit the Data\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'fit_intercept': True,\n",
       " 'loss': 'log',\n",
       " 'penalty': 'l2'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.72      0.74     48271\n",
      "          4       0.73      0.78      0.76     47729\n",
      "\n",
      "avg / total       0.75      0.75      0.75     96000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_prediction = grid.predict(X_test)\n",
    "print(classification_report(y_test, sgd_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7023125\n",
      "--- 838.5373966693878 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################ Gradient Boost #############################\n",
    "start_time = time.time()\n",
    "parameters = {'subsample':[1],\n",
    "              'max_depth':[2],\n",
    "              'loss':['exponential'],\n",
    "             'n_estimators':[500]}\n",
    "\n",
    "# Initialize the model.\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "#Create grid and perform 5 cross validation\n",
    "gradient_grid = GridSearchCV(clf, parameters, cv=3, verbose=0)\n",
    "\n",
    "#Fit the Data\n",
    "gradient_grid.fit(X_train, y_train)\n",
    "print(gradient_grid.score(X_test, y_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.67      0.69     23884\n",
      "          4       0.69      0.74      0.72     24116\n",
      "\n",
      "avg / total       0.71      0.71      0.71     48000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_prediction = grid.predict(X_test)\n",
    "print(classification_report(y_test, gb_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Random Forest #############################\n",
    "start_time = time.time()\n",
    "parameters = {\n",
    "               'n_estimators':[10,100,200],\n",
    "                'criterion':['gini','entropy'],\n",
    "                'max_depth':[None, 2, 4, 6],\n",
    "                'class_weight':[None, 'balanced'],\n",
    "               'max_features': ['auto']\n",
    "              }\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "random_forest_grid = GridSearchCV(rfc, parameters, scoring='accuracy', cv=3, verbose=0)\n",
    "#Fit the Data\n",
    "random_forest_grid.fit(X_train, y_train)\n",
    "print(random_forest_grid.score(X_test, y_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.65      0.69     47970\n",
      "          4       0.69      0.76      0.72     48030\n",
      "\n",
      "avg / total       0.71      0.71      0.71     96000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_prediction = grid.predict(X_test)\n",
    "print(classification_report(y_test, rf_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD proved to be the fastest and accurate model. Will now try to just act on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_tfidf,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7485208333333333\n",
      "-- Execution time: 2.2076778411865234 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################ SGD ######################\n",
    "start_time = time.time()\n",
    "parameters = {\n",
    "                'loss':['log'],\n",
    "                'penalty':['l2'],\n",
    "                'fit_intercept':[True],\n",
    "                'class_weight':['balanced']\n",
    "               \n",
    "              }\n",
    "\n",
    "clf = linear_model.SGDClassifier(n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(clf, parameters, scoring='accuracy', cv=5, verbose=0)\n",
    "#Fit the Data\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.71      0.74     48200\n",
      "          4       0.73      0.78      0.76     47800\n",
      "\n",
      "avg / total       0.75      0.75      0.75     96000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_prediction = grid.predict(X_test)\n",
    "print(classification_report(y_test, sgd_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining\n",
    "\n",
    "Since Im going to pick the SGD model, will streamline this process. I have already done a grid search to find the optmimal parameters for the different models, now I need to find the optimal settings for the vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 2130.344949245453 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',lowercase=True,use_idf=True,max_df=0.5)),\n",
    "                 ('svd', TruncatedSVD(500)),\n",
    "                 ('norm',Normalizer(copy=False)),\n",
    "                 ('sgd',linear_model.SGDClassifier(loss='log',penalty='l2',fit_intercept=True, class_weight='balanced', n_jobs=50))\n",
    "                 ])\n",
    "\n",
    "parameters = {'vectidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vectidf__min_df':(1,2),\n",
    "              'vectidf__norm':['l1','l2'],\n",
    "              'vectidf__smooth_idf':[True, False]    \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, parameters, n_jobs=50, cv=5, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7362875"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectidf__min_df': 2,\n",
       " 'vectidf__ngram_range': (1, 2),\n",
       " 'vectidf__norm': 'l2',\n",
       " 'vectidf__smooth_idf': False}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7387\n",
      "-- Execution time: 12.548697471618652 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(grid.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
